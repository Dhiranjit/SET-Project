{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874f53f9",
   "metadata": {},
   "source": [
    "### Process News Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc1c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Suppress a common but harmless warning from transformers\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def initialize_finbert():\n",
    "    \"\"\"\n",
    "    Initializes and returns the FinBERT sentiment analysis pipeline.\n",
    "    \"\"\"\n",
    "    device_name = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = 'cuda'\n",
    "    \n",
    "    print(f\"Initializing FinBERT pipeline... (Using device: {device_name})\")\n",
    "    \n",
    "    try:\n",
    "        sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=\"ProsusAI/finbert\", \n",
    "            device=-1 if device_name == 'cpu' else 0\n",
    "        )\n",
    "        print(\"FinBERT pipeline loaded successfully.\")\n",
    "        return sentiment_pipeline\n",
    "    except Exception as e:\n",
    "        print(f\"[Fatal Error] Could not load FinBERT model: {e}\")\n",
    "        print(\"Make sure you are connected to the internet and have 'transformers' and 'torch' installed.\")\n",
    "        sys.exit(1) # Exit the script if the model can't be loaded\n",
    "\n",
    "def process_news_file(news_file, sentiment_pipeline):\n",
    "    \"\"\"\n",
    "    Reads a raw news file, analyzes every article, and returns a DataFrame \n",
    "    of daily average sentiment scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Read and Clean News Data ---\n",
    "    try:\n",
    "        news_df = pd.read_csv(news_file)\n",
    "        news_df['date'] = pd.to_datetime(news_df['date'])\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Could not read or parse news file {news_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Clean text and create a 'full_text' column for analysis\n",
    "    news_df['description'] = news_df['description'].fillna('').astype(str)\n",
    "    news_df['title'] = news_df['title'].astype(str)\n",
    "    news_df['full_text'] = news_df['title'] + ' ' + news_df['description']\n",
    "    \n",
    "    # Filter out empty text\n",
    "    valid_texts_df = news_df[news_df['full_text'].str.strip() != ''].copy()\n",
    "    \n",
    "    if valid_texts_df.empty:\n",
    "        print(\"  > No valid news text found. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['date', 'avg_polarity', 'avg_confidence']).set_index('date')\n",
    "\n",
    "    # --- 2. Run FinBERT Analysis ---\n",
    "    print(f\"  > Analyzing {len(valid_texts_df)} individual news articles...\")\n",
    "    texts_to_analyze = valid_texts_df['full_text'].tolist()\n",
    "    try:\n",
    "        results = sentiment_pipeline(texts_to_analyze, truncation=True, batch_size=32)\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] FinBERT analysis failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Map results back to the DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    valid_texts_df['label'] = results_df['label'].values\n",
    "    valid_texts_df['confidence'] = results_df['score'].values\n",
    "    \n",
    "    # --- 3. Calculate Polarity Score ---\n",
    "    # Convert label to a single polarity score:\n",
    "    # positive -> +confidence, negative -> -confidence, neutral -> 0\n",
    "    def calculate_polarity(row):\n",
    "        if row['label'] == 'positive':\n",
    "            return row['confidence']\n",
    "        elif row['label'] == 'negative':\n",
    "            return -row['confidence']\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    valid_texts_df['polarity'] = valid_texts_df.apply(calculate_polarity, axis=1)\n",
    "    \n",
    "    # --- 4. Aggregate to get DAILY AVERAGE ---\n",
    "    print(\"  > Aggregating sentiment scores by day...\")\n",
    "    valid_texts_df.set_index('date', inplace=True)\n",
    "    daily_avg_sentiment = valid_texts_df.resample('D').agg(\n",
    "        avg_polarity=('polarity', 'mean'),\n",
    "        avg_confidence=('confidence', 'mean')\n",
    "    )\n",
    "    \n",
    "    # Fill any gaps (days with news but no valid text?) with 0s\n",
    "    daily_avg_sentiment.fillna(0, inplace=True)\n",
    "    \n",
    "    return daily_avg_sentiment\n",
    "\n",
    "def main():\n",
    "    # --- Configuration ---\n",
    "    STOCK_NEWS_DIR = \"Stock News\"\n",
    "    OUTPUT_DIR = \"daily_sentiment_data\" # New folder for this module's output\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Load the model ONCE for the entire run\n",
    "    sentiment_pipeline = initialize_finbert()\n",
    "    \n",
    "    search_path = os.path.join(STOCK_NEWS_DIR, \"*_news.csv\")\n",
    "    news_files = glob.glob(search_path)\n",
    "    \n",
    "    if not news_files:\n",
    "        print(f\"No *_news.csv files found in '{STOCK_NEWS_DIR}' folder.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(news_files)} stocks. Starting sentiment processing...\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    for news_file in news_files:\n",
    "        basename = os.path.basename(news_file)\n",
    "        ticker = basename.split('_news.csv')[0]\n",
    "        \n",
    "        print(f\"\\n--- Processing: {ticker} ---\")\n",
    "            \n",
    "        daily_sentiment_data = process_news_file(news_file, sentiment_pipeline)\n",
    "        \n",
    "        if daily_sentiment_data is not None:\n",
    "            # Reset index to make 'date' a column\n",
    "            daily_sentiment_data.reset_index(inplace=True)\n",
    "            \n",
    "            output_filename = os.path.join(OUTPUT_DIR, f\"{ticker}_sentiment.csv\")\n",
    "            daily_sentiment_data.to_csv(output_filename, index=False)\n",
    "            \n",
    "            print(f\"  [Success] Saved to: {output_filename}\")\n",
    "            print(f\"  > Total days with sentiment: {len(daily_sentiment_data)}\")\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"  [Failed] Could not process {ticker}.\")\n",
    "            fail_count += 1\n",
    "\n",
    "    print(f\"\\n--- Sentiment Processing Complete ---\")\n",
    "    print(f\"Successfully processed: {success_count} stocks\")\n",
    "    print(f\"Skipped/Failed: {fail_count} stocks\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e9f21",
   "metadata": {},
   "source": [
    "### Creating the Final Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5590c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "def process_features(sentiment_file, news_file):\n",
    "    \"\"\"\n",
    "    Combines daily sentiment with daily news counts and engineers new\n",
    "    features like change, momentum, and count.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Load Daily Sentiment Data ---\n",
    "    try:\n",
    "        sentiment_df = pd.read_csv(sentiment_file, parse_dates=['date'])\n",
    "        sentiment_df.set_index('date', inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Could not read sentiment file {sentiment_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. Load and Aggregate News Count ---\n",
    "    try:\n",
    "        news_df = pd.read_csv(news_file, parse_dates=['date'])\n",
    "        news_df.set_index('date', inplace=True)\n",
    "        # Resample by day ('D') and get the size (count) of each group\n",
    "        count_df = news_df.resample('D').size().to_frame('news_article_count')\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Could not read or process news file {news_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. Merge Sentiment and Counts ---\n",
    "    # We join to the sentiment_df, which has the full daily index\n",
    "    print(\"  > Merging sentiment scores and article counts...\")\n",
    "    merged_df = sentiment_df.join(count_df, how='left')\n",
    "    \n",
    "    # Fill days with no news with 0 articles\n",
    "    merged_df['news_article_count'].fillna(0, inplace=True)\n",
    "    merged_df['news_article_count'] = merged_df['news_article_count'].astype(int)\n",
    "    \n",
    "    # Ensure data is sorted by date for correct momentum/change calculation\n",
    "    merged_df.sort_index(inplace=True)\n",
    "\n",
    "    # --- 4. Engineer New Features ---\n",
    "    print(\"  > Engineering features: change, momentum, count...\")\n",
    "    \n",
    "    # 1. sentiment_change: Change in sentiment score from previous day\n",
    "    # .diff() calculates the difference from the previous row\n",
    "    merged_df['sentiment_change'] = merged_df['avg_polarity'].diff().fillna(0)\n",
    "    \n",
    "    # 2. sentiment_momentum: Moving average of sentiment (3-day and 5-day)\n",
    "    # .rolling() creates a sliding window for calculations\n",
    "    merged_df['momentum_3d'] = merged_df['avg_polarity'].rolling(window=3).mean().fillna(0)\n",
    "    merged_df['momentum_5d'] = merged_df['avg_polarity'].rolling(window=5).mean().fillna(0)\n",
    "    \n",
    "    # 3. news_article_count: (Already created in step 3)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def main():\n",
    "    # --- Configuration ---\n",
    "    INPUT_SENTIMENT_DIR = \"daily_sentiment_data\"\n",
    "    INPUT_NEWS_DIR = \"Stock News\"\n",
    "    OUTPUT_DIR = \"final_sentiment_features\" # New folder for this module's output\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Find all the sentiment files we just created\n",
    "    search_path = os.path.join(INPUT_SENTIMENT_DIR, \"*_sentiment.csv\")\n",
    "    sentiment_files = glob.glob(search_path)\n",
    "    \n",
    "    if not sentiment_files:\n",
    "        print(f\"No *_sentiment.csv files found in '{INPUT_SENTIMENT_DIR}' folder.\")\n",
    "        print(\"Please run 'process_news_sentiment.py' first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(sentiment_files)} sentiment files. Starting feature engineering...\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    for sentiment_file in sentiment_files:\n",
    "        basename = os.path.basename(sentiment_file)\n",
    "        ticker = basename.split('_sentiment.csv')[0]\n",
    "        \n",
    "        print(f\"\\n--- Processing: {ticker} ---\")\n",
    "        \n",
    "        # Find the matching original news file\n",
    "        news_file = os.path.join(INPUT_NEWS_DIR, f\"{ticker}_news.csv\")\n",
    "        \n",
    "        if not os.path.exists(news_file):\n",
    "            print(f\"  [Skipped] No matching raw news file found at: {news_file}\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "            \n",
    "        feature_data = process_features(sentiment_file, news_file)\n",
    "        \n",
    "        if feature_data is not None:\n",
    "            # Reset index to make 'date' a column again\n",
    "            feature_data.reset_index(inplace=True)\n",
    "            \n",
    "            output_filename = os.path.join(OUTPUT_DIR, f\"{ticker}_features.csv\")\n",
    "            feature_data.to_csv(output_filename, index=False)\n",
    "            \n",
    "            print(f\"  [Success] Saved to: {output_filename}\")\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"  [Failed] Could not process {ticker}.\")\n",
    "            fail_count += 1\n",
    "\n",
    "    print(f\"\\n--- Feature Engineering Complete ---\")\n",
    "    print(f\"Successfully processed: {success_count} stocks\")\n",
    "    print(f\"Skipped/Failed: {fail_count} stocks\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
